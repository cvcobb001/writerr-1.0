{
  "version": 3,
  "sources": ["main.ts", "src/TokenCalculator.ts", "src/TokenLimitService.ts", "src/TokenAPIService.ts", "src/settings.ts"],
  "sourcesContent": ["import { Plugin } from 'obsidian';\nimport { TokenCalculator } from './src/TokenCalculator';\nimport { TokenLimitService } from './src/TokenLimitService';\nimport { TokenAPIService } from './src/TokenAPIService';\nimport { TokenCountSettings, DEFAULT_SETTINGS, TokenCountSettingTab } from './src/settings';\n\nexport default class TokenCountPlugin extends Plugin {\n  settings: TokenCountSettings;\n  tokenCalculator: TokenCalculator;\n  tokenLimitService: TokenLimitService;\n  api: TokenAPIService;\n  \n  async onload() {\n    // Load settings\n    await this.loadSettings();\n\n    if (this.settings.enableDebugLogs) {\n      console.log('\uD83D\uDD22 Loading Token Count plugin...');\n    }\n\n    // Initialize core services\n    this.tokenCalculator = new TokenCalculator();\n    this.tokenLimitService = new TokenLimitService(this.settings);\n    \n    // Initialize external data\n    try {\n      await this.tokenLimitService.initialize();\n    } catch (error) {\n      if (this.settings.enableDebugLogs) {\n        console.warn('Token Count: Failed to load model data:', error);\n      }\n    }\n\n    // Setup public API (pure service - no UI commands)\n    this.api = new TokenAPIService(this.tokenCalculator, this.tokenLimitService);\n    \n    // Add settings tab\n    this.addSettingTab(new TokenCountSettingTab(this.app, this));\n\n    if (this.settings.enableDebugLogs) {\n      console.log('\u2705 Token Count plugin loaded successfully');\n      \n      // Log cache info in debug mode\n      const cacheInfo = this.api.getCacheInfo();\n      console.log(`\uD83D\uDCCA Token Count: ${cacheInfo.modelCount} models loaded, expires: ${cacheInfo.expiresAt.toLocaleString()}`);\n    }\n  }\n\n  onunload() {\n    if (this.settings.enableDebugLogs) {\n      console.log('\uD83D\uDD22 Token Count plugin unloaded');\n    }\n  }\n\n  async loadSettings() {\n    this.settings = Object.assign({}, DEFAULT_SETTINGS, await this.loadData());\n  }\n\n  async saveSettings() {\n    await this.saveData(this.settings);\n    \n    // Update services when settings change\n    if (this.tokenLimitService) {\n      this.tokenLimitService.updateSettings(this.settings);\n    }\n  }\n}", "/**\n * TokenCalculator - Sophisticated model-specific token counting\n * Extracted from Writerr Chat plugin tokenization system\n */\n\nexport class TokenCalculator {\n  \n  /**\n   * Calculate tokens for a specific model using appropriate tokenizer\n   */\n  calculateTokensForModel(text: string, modelName: string): number {\n    // Clean and normalize text\n    const normalizedText = text.trim();\n    if (!normalizedText) return 0;\n\n    // Determine tokenizer type based on model\n    const tokenizerType = this.getTokenizerType(modelName);\n    \n    switch (tokenizerType) {\n      case 'cl100k':\n        return this.cl100kTokenizer(normalizedText);\n      case 'p50k':\n        return this.p50kTokenizer(normalizedText);\n      case 'gemini':\n        return this.geminiTokenizer(normalizedText);\n      case 'claude':\n        return this.claudeTokenizer(normalizedText);\n      default:\n        return this.fallbackTokenizer(normalizedText);\n    }\n  }\n\n  /**\n   * Determine which tokenizer to use based on model name\n   */\n  getTokenizerType(modelName: string): string {\n    const modelLower = modelName.toLowerCase();\n    \n    // OpenAI GPT-4, GPT-4o, GPT-5, o1, o3, o4 use cl100k_base\n    if (modelLower.includes('gpt-4') || modelLower.includes('gpt-5') || \n        modelLower.includes('gpt-4o') || modelLower.includes('o1') || \n        modelLower.includes('o3') || modelLower.includes('o4')) {\n      return 'cl100k';\n    }\n    \n    // OpenAI GPT-3.5 and older use p50k_base\n    if (modelLower.includes('gpt-3') || modelLower.includes('davinci') || \n        modelLower.includes('babbage') || modelLower.includes('curie')) {\n      return 'p50k';\n    }\n    \n    // Google Gemini models\n    if (modelLower.includes('gemini') || modelLower.includes('models/gemini')) {\n      return 'gemini';\n    }\n    \n    // Anthropic Claude models\n    if (modelLower.includes('claude')) {\n      return 'claude';\n    }\n    \n    return 'cl100k'; // Default to most common modern tokenizer\n  }\n\n  /**\n   * cl100k_base tokenizer approximation for GPT-4/GPT-4o/GPT-5/o1/o3/o4\n   * Most sophisticated tokenizer for modern OpenAI models\n   */\n  cl100kTokenizer(text: string): number {\n    // Step 1: Handle special tokens and patterns\n    let tokenCount = 0;\n    \n    // Count newlines (each newline is typically 1 token)\n    const newlines = (text.match(/\\n/g) || []).length;\n    tokenCount += newlines;\n    \n    // Remove newlines for further processing\n    let processedText = text.replace(/\\n/g, ' ');\n    \n    // Step 2: Split on whitespace and punctuation\n    const words = processedText.split(/\\s+/).filter(word => word.length > 0);\n    \n    for (const word of words) {\n      // Handle punctuation-heavy text\n      if (/^[^\\w\\s]+$/.test(word)) {\n        // Pure punctuation - usually 1 token per character or small group\n        tokenCount += Math.ceil(word.length / 2);\n      } else if (word.length <= 3) {\n        // Short words are typically 1 token\n        tokenCount += 1;\n      } else if (word.length <= 7) {\n        // Medium words are typically 1-2 tokens\n        tokenCount += Math.ceil(word.length / 4);\n      } else {\n        // Long words get split more\n        tokenCount += Math.ceil(word.length / 3.5);\n      }\n    }\n    \n    return Math.max(1, tokenCount);\n  }\n\n  /**\n   * p50k_base tokenizer approximation for GPT-3.5 and older\n   * Slightly less efficient than cl100k\n   */\n  p50kTokenizer(text: string): number {\n    const words = text.split(/\\s+/).filter(word => word.length > 0);\n    let tokenCount = 0;\n    \n    for (const word of words) {\n      if (word.length <= 4) {\n        tokenCount += 1;\n      } else {\n        // p50k is less efficient, so slightly higher token count\n        tokenCount += Math.ceil(word.length / 3.8);\n      }\n    }\n    \n    // Add newline tokens\n    tokenCount += (text.match(/\\n/g) || []).length;\n    \n    return Math.max(1, tokenCount);\n  }\n\n  /**\n   * Google Gemini tokenizer approximation\n   * Generally more efficient than GPT tokenizers\n   */\n  geminiTokenizer(text: string): number {\n    const words = text.split(/\\s+/).filter(word => word.length > 0);\n    let tokenCount = 0;\n    \n    for (const word of words) {\n      if (word.length <= 4) {\n        tokenCount += 1;\n      } else {\n        // Gemini is typically more efficient\n        tokenCount += Math.ceil(word.length / 4.2);\n      }\n    }\n    \n    // Handle newlines\n    tokenCount += (text.match(/\\n/g) || []).length * 0.8; // Gemini handles newlines more efficiently\n    \n    return Math.max(1, Math.ceil(tokenCount));\n  }\n\n  /**\n   * Anthropic Claude tokenizer approximation\n   * Similar efficiency to GPT-4 family\n   */\n  claudeTokenizer(text: string): number {\n    const words = text.split(/\\s+/).filter(word => word.length > 0);\n    let tokenCount = 0;\n    \n    for (const word of words) {\n      if (word.length <= 3) {\n        tokenCount += 1;\n      } else {\n        tokenCount += Math.ceil(word.length / 3.7);\n      }\n    }\n    \n    // Handle newlines\n    tokenCount += (text.match(/\\n/g) || []).length;\n    \n    return Math.max(1, tokenCount);\n  }\n\n  /**\n   * Conservative fallback estimation for unknown models\n   */\n  fallbackTokenizer(text: string): number {\n    // Conservative fallback estimation\n    return Math.ceil(text.length / 4);\n  }\n\n  /**\n   * Easy interface - auto-detect model or use fallback\n   */\n  estimateTokens(text: string, modelName?: string): number {\n    if (!text) return 0;\n    \n    if (!modelName) {\n      return this.fallbackTokenizer(text);\n    }\n\n    return this.calculateTokensForModel(text, modelName);\n  }\n}", "/**\n * TokenLimitService - Real-time model capacity data from configurable API\n * Provides accurate token limits without hardcoded fallbacks\n */\n\nimport { TokenCountSettings } from './settings';\n\nexport interface ModelInfo {\n  id: string;\n  name: string;\n  contextLength: number;\n  pricing?: {\n    prompt: number;\n    completion: number;\n  };\n}\n\nexport class TokenLimitService {\n  private cache: Map<string, ModelInfo> = new Map();\n  private cacheExpiry: number = 0;\n  private settings: TokenCountSettings;\n\n  constructor(settings: TokenCountSettings) {\n    this.settings = settings;\n  }\n\n  /**\n   * Initialize service - fetch model data if needed\n   */\n  async initialize(): Promise<void> {\n    if (this.settings.enableDebugLogs) {\n      console.log('\uD83D\uDD0D Initializing TokenLimitService...');\n    }\n    \n    // Try to load from localStorage first\n    this.loadFromCache();\n    \n    // Only fetch from external API if enabled\n    if (this.settings.enableExternalAPIs && !this.isDataFresh()) {\n      try {\n        await this.fetchModelLimits();\n        if (this.settings.enableDebugLogs) {\n          console.log('\u2705 Model data refreshed from external API');\n        }\n      } catch (error) {\n        if (this.settings.enableDebugLogs) {\n          console.warn('\u26A0\uFE0F Failed to fetch fresh model data, using cache:', error);\n        }\n      }\n    } else if (this.settings.enableDebugLogs) {\n      console.log('\u2705 Using cached model data or external APIs disabled');\n    }\n  }\n\n  /**\n   * Fetch fresh model data from configured API\n   */\n  async fetchModelLimits(): Promise<void> {\n    if (!this.settings.enableExternalAPIs) {\n      throw new Error('External API calls are disabled in settings');\n    }\n\n    try {\n      if (this.settings.enableDebugLogs) {\n        console.log('\uD83D\uDCE1 Fetching model limits from:', this.settings.apiEndpoint);\n      }\n      \n      const response = await fetch(this.settings.apiEndpoint);\n      if (!response.ok) {\n        throw new Error(`API failed: ${response.status}`);\n      }\n      \n      const data = await response.json();\n      \n      if (!data.data || !Array.isArray(data.data)) {\n        throw new Error('Invalid response format from OpenRouter');\n      }\n\n      // Process and cache models\n      let processedCount = 0;\n      for (const model of data.data) {\n        if (model.context_length && model.id) {\n          const modelInfo: ModelInfo = {\n            id: model.id,\n            name: model.name || model.id,\n            contextLength: model.context_length,\n            pricing: model.pricing ? {\n              prompt: model.pricing.prompt,\n              completion: model.pricing.completion\n            } : undefined\n          };\n\n          // Store with multiple keys for easier lookup\n          this.cache.set(model.id, modelInfo);\n          this.cache.set(this.normalizeModelName(model.id), modelInfo);\n          this.cache.set(this.normalizeModelName(model.name || model.id), modelInfo);\n          \n          processedCount++;\n        }\n      }\n\n      // Update cache timestamp with configurable duration\n      const cacheDuration = this.settings.cacheExpiration * 60 * 60 * 1000; // Convert hours to milliseconds\n      this.cacheExpiry = Date.now() + cacheDuration;\n      \n      // Persist to localStorage\n      this.saveToCache();\n      \n      if (this.settings.enableDebugLogs) {\n        console.log(`\u2705 Processed ${processedCount} models from API`);\n      }\n      \n    } catch (error) {\n      console.error('\u274C Failed to fetch model limits:', error);\n      throw error;\n    }\n  }\n\n  /**\n   * Get token limit for a specific model\n   */\n  getTokenLimit(modelName: string): number | null {\n    if (!modelName) return null;\n\n    // Try exact match first\n    const exact = this.cache.get(modelName);\n    if (exact) return exact.contextLength;\n\n    // Try normalized match\n    const normalized = this.cache.get(this.normalizeModelName(modelName));\n    if (normalized) return normalized.contextLength;\n\n    // Try fuzzy matching for common patterns\n    const fuzzyMatch = this.fuzzyMatch(modelName);\n    if (fuzzyMatch) return fuzzyMatch.contextLength;\n\n    return null;\n  }\n\n  /**\n   * Get full model info including pricing\n   */\n  getModelInfo(modelName: string): ModelInfo | null {\n    if (!modelName) return null;\n\n    // Try exact match first\n    const exact = this.cache.get(modelName);\n    if (exact) return exact;\n\n    // Try normalized match\n    const normalized = this.cache.get(this.normalizeModelName(modelName));\n    if (normalized) return normalized;\n\n    // Try fuzzy matching\n    return this.fuzzyMatch(modelName);\n  }\n\n  /**\n   * Get all available models\n   */\n  getSupportedModels(): ModelInfo[] {\n    const uniqueModels = new Map<string, ModelInfo>();\n    \n    // Deduplicate by ID\n    for (const model of this.cache.values()) {\n      uniqueModels.set(model.id, model);\n    }\n    \n    return Array.from(uniqueModels.values()).sort((a, b) => a.name.localeCompare(b.name));\n  }\n\n  /**\n   * Normalize model name for better matching\n   */\n  private normalizeModelName(modelName: string): string {\n    return modelName\n      .toLowerCase()\n      .replace(/[^a-z0-9]/g, '-')  // Replace non-alphanumeric with dashes\n      .replace(/-+/g, '-')         // Collapse multiple dashes\n      .replace(/^-|-$/g, '');      // Remove leading/trailing dashes\n  }\n\n  /**\n   * Fuzzy matching for common model name variations\n   */\n  private fuzzyMatch(modelName: string): ModelInfo | null {\n    const normalized = this.normalizeModelName(modelName);\n    \n    // Check for partial matches\n    for (const [key, model] of this.cache.entries()) {\n      const keyNormalized = this.normalizeModelName(key);\n      \n      // Check if normalized names contain each other\n      if (keyNormalized.includes(normalized) || normalized.includes(keyNormalized)) {\n        return model;\n      }\n    }\n    \n    return null;\n  }\n\n  /**\n   * Check if cached data is still fresh\n   */\n  private isDataFresh(): boolean {\n    return this.cache.size > 0 && Date.now() < this.cacheExpiry;\n  }\n\n  /**\n   * Save cache to localStorage\n   */\n  private saveToCache(): void {\n    try {\n      const cacheData = {\n        models: Array.from(this.cache.entries()),\n        expiry: this.cacheExpiry\n      };\n      \n      localStorage.setItem('token-count-cache', JSON.stringify(cacheData));\n    } catch (error) {\n      console.warn('Failed to save token cache:', error);\n    }\n  }\n\n  /**\n   * Load cache from localStorage\n   */\n  private loadFromCache(): void {\n    try {\n      const cached = localStorage.getItem('token-count-cache');\n      if (!cached) return;\n      \n      const cacheData = JSON.parse(cached);\n      \n      // Check if cache is still valid\n      if (Date.now() > cacheData.expiry) {\n        localStorage.removeItem('token-count-cache');\n        return;\n      }\n      \n      // Restore cache\n      this.cache = new Map(cacheData.models);\n      this.cacheExpiry = cacheData.expiry;\n      \n      console.log(`\uD83D\uDCE6 Loaded ${this.cache.size} models from cache`);\n      \n    } catch (error) {\n      console.warn('Failed to load token cache:', error);\n      localStorage.removeItem('token-count-cache');\n    }\n  }\n\n  /**\n   * Force refresh of model data\n   */\n  async refresh(): Promise<void> {\n    this.cache.clear();\n    this.cacheExpiry = 0;\n    await this.fetchModelLimits();\n  }\n\n  /**\n   * Update settings and potentially refresh data\n   */\n  updateSettings(settings: TokenCountSettings): void {\n    this.settings = settings;\n    \n    // If cache duration changed, update expiry\n    const cacheDuration = this.settings.cacheExpiration * 60 * 60 * 1000;\n    this.cacheExpiry = Date.now() + cacheDuration;\n  }\n\n  /**\n   * Get cache statistics\n   */\n  getCacheInfo(): { modelCount: number; isExpired: boolean; expiresAt: Date } {\n    return {\n      modelCount: this.cache.size,\n      isExpired: !this.isDataFresh(),\n      expiresAt: new Date(this.cacheExpiry)\n    };\n  }\n}", "/**\n * TokenAPIService - Public API for other plugins to consume\n * Clean interface for token analysis and model data - Pure service, no UI\n */\n\nimport { TokenCalculator } from './TokenCalculator';\nimport { TokenLimitService, ModelInfo } from './TokenLimitService';\n\nexport interface TokenAnalysis {\n  tokenCount: number;\n  modelName: string;\n  tokenLimit: number | null;\n  percentage: number;\n  estimatedCost: number | null;\n  isEstimate: boolean;\n}\n\nexport class TokenAPIService {\n  constructor(\n    private tokenCalculator: TokenCalculator,\n    private tokenLimitService: TokenLimitService\n  ) {}\n\n  /**\n   * Analyze text for token usage - main API method\n   */\n  analyzeText(text: string, modelName?: string): TokenAnalysis {\n    if (!text) {\n      return {\n        tokenCount: 0,\n        modelName: modelName || 'unknown',\n        tokenLimit: null,\n        percentage: 0,\n        estimatedCost: null,\n        isEstimate: true\n      };\n    }\n\n    // Calculate tokens using sophisticated algorithms\n    const tokenCount = modelName \n      ? this.tokenCalculator.calculateTokensForModel(text, modelName)\n      : this.tokenCalculator.estimateTokens(text);\n\n    // Get model capacity data\n    const tokenLimit = modelName ? this.tokenLimitService.getTokenLimit(modelName) : null;\n    const modelInfo = modelName ? this.tokenLimitService.getModelInfo(modelName) : null;\n    \n    // Calculate percentage if we have limits\n    const percentage = tokenLimit ? (tokenCount / tokenLimit) * 100 : 0;\n    \n    // Estimate cost if we have pricing data\n    let estimatedCost = null;\n    if (modelInfo?.pricing) {\n      // Rough cost estimation (assumes mostly prompt tokens)\n      estimatedCost = (tokenCount / 1000) * modelInfo.pricing.prompt;\n    }\n\n    return {\n      tokenCount,\n      modelName: modelName || 'unknown',\n      tokenLimit,\n      percentage,\n      estimatedCost,\n      isEstimate: true // Always true for pre-request estimates\n    };\n  }\n\n  /**\n   * Get token limit for a specific model\n   */\n  getTokenLimit(modelName: string): number | null {\n    return this.tokenLimitService.getTokenLimit(modelName);\n  }\n\n  /**\n   * Get full model information including pricing\n   */\n  getModelInfo(modelName: string): ModelInfo | null {\n    return this.tokenLimitService.getModelInfo(modelName);\n  }\n\n  /**\n   * Get list of all supported models\n   */\n  getSupportedModels(): ModelInfo[] {\n    return this.tokenLimitService.getSupportedModels();\n  }\n\n  /**\n   * Calculate cost for a given number of tokens\n   */\n  calculateCost(tokens: number, modelName: string, completionTokens: number = 0): number | null {\n    const modelInfo = this.tokenLimitService.getModelInfo(modelName);\n    if (!modelInfo?.pricing) return null;\n\n    const promptTokens = tokens - completionTokens;\n    const promptCost = (promptTokens / 1000) * modelInfo.pricing.prompt;\n    const completionCost = (completionTokens / 1000) * modelInfo.pricing.completion;\n    \n    return promptCost + completionCost;\n  }\n\n\n  /**\n   * Quick token count without model info\n   */\n  quickCount(text: string): number {\n    return this.tokenCalculator.estimateTokens(text);\n  }\n\n  /**\n   * Refresh model data from external sources\n   */\n  async refreshModelData(): Promise<void> {\n    await this.tokenLimitService.refresh();\n  }\n\n  /**\n   * Get cache information\n   */\n  getCacheInfo(): { modelCount: number; isExpired: boolean; expiresAt: Date } {\n    return this.tokenLimitService.getCacheInfo();\n  }\n}", "import { App, PluginSettingTab, Setting } from 'obsidian';\nimport TokenCountPlugin from '../main';\n\nexport interface TokenCountSettings {\n  enableExternalAPIs: boolean;\n  apiEndpoint: string;\n  cacheExpiration: number; // hours\n  fallbackMode: 'unavailable' | 'basic-estimates';\n  enableDebugLogs: boolean;\n}\n\nexport const DEFAULT_SETTINGS: TokenCountSettings = {\n  enableExternalAPIs: true,\n  apiEndpoint: 'https://openrouter.ai/api/v1/models',\n  cacheExpiration: 24,\n  fallbackMode: 'unavailable',\n  enableDebugLogs: false\n};\n\nexport class TokenCountSettingTab extends PluginSettingTab {\n  plugin: TokenCountPlugin;\n\n  constructor(app: App, plugin: TokenCountPlugin) {\n    super(app, plugin);\n    this.plugin = plugin;\n  }\n\n  display(): void {\n    const { containerEl } = this;\n    containerEl.empty();\n\n    containerEl.createEl('h2', { text: 'Token Count Settings' });\n\n    // External API toggle\n    new Setting(containerEl)\n      .setName('Enable external API calls')\n      .setDesc('Allow fetching model data from external sources. Disable for privacy or offline use.')\n      .addToggle(toggle => toggle\n        .setValue(this.plugin.settings.enableExternalAPIs)\n        .onChange(async (value) => {\n          this.plugin.settings.enableExternalAPIs = value;\n          await this.plugin.saveSettings();\n          \n          // Refresh display to show/hide API-related settings\n          this.display();\n        }));\n\n    if (this.plugin.settings.enableExternalAPIs) {\n      // API endpoint setting\n      new Setting(containerEl)\n        .setName('API endpoint')\n        .setDesc('URL for fetching model token limits. Default: OpenRouter API')\n        .addText(text => text\n          .setPlaceholder('https://openrouter.ai/api/v1/models')\n          .setValue(this.plugin.settings.apiEndpoint)\n          .onChange(async (value) => {\n            this.plugin.settings.apiEndpoint = value || DEFAULT_SETTINGS.apiEndpoint;\n            await this.plugin.saveSettings();\n          }));\n\n      // Cache expiration\n      new Setting(containerEl)\n        .setName('Cache expiration')\n        .setDesc('Hours to cache model data before refreshing')\n        .addSlider(slider => slider\n          .setLimits(1, 168, 1) // 1 hour to 1 week\n          .setValue(this.plugin.settings.cacheExpiration)\n          .setDynamicTooltip()\n          .onChange(async (value) => {\n            this.plugin.settings.cacheExpiration = value;\n            await this.plugin.saveSettings();\n          }));\n    }\n\n    // Fallback mode\n    new Setting(containerEl)\n      .setName('Fallback mode')\n      .setDesc('What to show when model token limits are unknown')\n      .addDropdown(dropdown => dropdown\n        .addOption('unavailable', 'Show \"unavailable\" (honest)')\n        .addOption('basic-estimates', 'Show basic estimates (less accurate)')\n        .setValue(this.plugin.settings.fallbackMode)\n        .onChange(async (value: 'unavailable' | 'basic-estimates') => {\n          this.plugin.settings.fallbackMode = value;\n          await this.plugin.saveSettings();\n        }));\n\n    // Debug logs toggle\n    new Setting(containerEl)\n      .setName('Enable debug logging')\n      .setDesc('Show detailed logs in developer console (for troubleshooting)')\n      .addToggle(toggle => toggle\n        .setValue(this.plugin.settings.enableDebugLogs)\n        .onChange(async (value) => {\n          this.plugin.settings.enableDebugLogs = value;\n          await this.plugin.saveSettings();\n        }));\n\n    // Cache info section\n    containerEl.createEl('h3', { text: 'Cache Status' });\n    \n    const cacheInfo = this.plugin.api?.getCacheInfo();\n    if (cacheInfo) {\n      const cacheDiv = containerEl.createEl('div', { cls: 'token-cache-info' });\n      \n      cacheDiv.createEl('p', { \n        text: `Models cached: ${cacheInfo.modelCount}` \n      });\n      \n      cacheDiv.createEl('p', { \n        text: `Cache expires: ${cacheInfo.expiresAt.toLocaleString()}` \n      });\n      \n      if (cacheInfo.isExpired) {\n        cacheDiv.createEl('p', { \n          text: '\u26A0\uFE0F Cache is expired', \n          cls: 'token-cache-warning' \n        });\n      }\n\n      // Manual refresh button\n      new Setting(containerEl)\n        .setName('Refresh model data')\n        .setDesc('Force refresh of model token limits from API')\n        .addButton(button => button\n          .setButtonText('Refresh Now')\n          .onClick(async () => {\n            if (this.plugin.api) {\n              try {\n                button.setButtonText('Refreshing...');\n                button.setDisabled(true);\n                \n                await this.plugin.api.refreshModelData();\n                \n                // Refresh the display to show updated cache info\n                this.display();\n              } catch (error) {\n                console.error('Failed to refresh model data:', error);\n                button.setButtonText('Refresh Failed');\n                setTimeout(() => {\n                  button.setButtonText('Refresh Now');\n                  button.setDisabled(false);\n                }, 2000);\n              }\n            }\n          }));\n    }\n  }\n}"],
  "mappings": ";;;;;;;;;;;;;;;;;;;;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,IAAAA,mBAAuB;;;ACKhB,IAAM,kBAAN,MAAsB;AAAA;AAAA;AAAA;AAAA,EAK3B,wBAAwB,MAAc,WAA2B;AAE/D,UAAM,iBAAiB,KAAK,KAAK;AACjC,QAAI,CAAC;AAAgB,aAAO;AAG5B,UAAM,gBAAgB,KAAK,iBAAiB,SAAS;AAErD,YAAQ,eAAe;AAAA,MACrB,KAAK;AACH,eAAO,KAAK,gBAAgB,cAAc;AAAA,MAC5C,KAAK;AACH,eAAO,KAAK,cAAc,cAAc;AAAA,MAC1C,KAAK;AACH,eAAO,KAAK,gBAAgB,cAAc;AAAA,MAC5C,KAAK;AACH,eAAO,KAAK,gBAAgB,cAAc;AAAA,MAC5C;AACE,eAAO,KAAK,kBAAkB,cAAc;AAAA,IAChD;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,iBAAiB,WAA2B;AAC1C,UAAM,aAAa,UAAU,YAAY;AAGzC,QAAI,WAAW,SAAS,OAAO,KAAK,WAAW,SAAS,OAAO,KAC3D,WAAW,SAAS,QAAQ,KAAK,WAAW,SAAS,IAAI,KACzD,WAAW,SAAS,IAAI,KAAK,WAAW,SAAS,IAAI,GAAG;AAC1D,aAAO;AAAA,IACT;AAGA,QAAI,WAAW,SAAS,OAAO,KAAK,WAAW,SAAS,SAAS,KAC7D,WAAW,SAAS,SAAS,KAAK,WAAW,SAAS,OAAO,GAAG;AAClE,aAAO;AAAA,IACT;AAGA,QAAI,WAAW,SAAS,QAAQ,KAAK,WAAW,SAAS,eAAe,GAAG;AACzE,aAAO;AAAA,IACT;AAGA,QAAI,WAAW,SAAS,QAAQ,GAAG;AACjC,aAAO;AAAA,IACT;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,gBAAgB,MAAsB;AAEpC,QAAI,aAAa;AAGjB,UAAM,YAAY,KAAK,MAAM,KAAK,KAAK,CAAC,GAAG;AAC3C,kBAAc;AAGd,QAAI,gBAAgB,KAAK,QAAQ,OAAO,GAAG;AAG3C,UAAM,QAAQ,cAAc,MAAM,KAAK,EAAE,OAAO,UAAQ,KAAK,SAAS,CAAC;AAEvE,eAAW,QAAQ,OAAO;AAExB,UAAI,aAAa,KAAK,IAAI,GAAG;AAE3B,sBAAc,KAAK,KAAK,KAAK,SAAS,CAAC;AAAA,MACzC,WAAW,KAAK,UAAU,GAAG;AAE3B,sBAAc;AAAA,MAChB,WAAW,KAAK,UAAU,GAAG;AAE3B,sBAAc,KAAK,KAAK,KAAK,SAAS,CAAC;AAAA,MACzC,OAAO;AAEL,sBAAc,KAAK,KAAK,KAAK,SAAS,GAAG;AAAA,MAC3C;AAAA,IACF;AAEA,WAAO,KAAK,IAAI,GAAG,UAAU;AAAA,EAC/B;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,cAAc,MAAsB;AAClC,UAAM,QAAQ,KAAK,MAAM,KAAK,EAAE,OAAO,UAAQ,KAAK,SAAS,CAAC;AAC9D,QAAI,aAAa;AAEjB,eAAW,QAAQ,OAAO;AACxB,UAAI,KAAK,UAAU,GAAG;AACpB,sBAAc;AAAA,MAChB,OAAO;AAEL,sBAAc,KAAK,KAAK,KAAK,SAAS,GAAG;AAAA,MAC3C;AAAA,IACF;AAGA,mBAAe,KAAK,MAAM,KAAK,KAAK,CAAC,GAAG;AAExC,WAAO,KAAK,IAAI,GAAG,UAAU;AAAA,EAC/B;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,gBAAgB,MAAsB;AACpC,UAAM,QAAQ,KAAK,MAAM,KAAK,EAAE,OAAO,UAAQ,KAAK,SAAS,CAAC;AAC9D,QAAI,aAAa;AAEjB,eAAW,QAAQ,OAAO;AACxB,UAAI,KAAK,UAAU,GAAG;AACpB,sBAAc;AAAA,MAChB,OAAO;AAEL,sBAAc,KAAK,KAAK,KAAK,SAAS,GAAG;AAAA,MAC3C;AAAA,IACF;AAGA,mBAAe,KAAK,MAAM,KAAK,KAAK,CAAC,GAAG,SAAS;AAEjD,WAAO,KAAK,IAAI,GAAG,KAAK,KAAK,UAAU,CAAC;AAAA,EAC1C;AAAA;AAAA;AAAA;AAAA;AAAA,EAMA,gBAAgB,MAAsB;AACpC,UAAM,QAAQ,KAAK,MAAM,KAAK,EAAE,OAAO,UAAQ,KAAK,SAAS,CAAC;AAC9D,QAAI,aAAa;AAEjB,eAAW,QAAQ,OAAO;AACxB,UAAI,KAAK,UAAU,GAAG;AACpB,sBAAc;AAAA,MAChB,OAAO;AACL,sBAAc,KAAK,KAAK,KAAK,SAAS,GAAG;AAAA,MAC3C;AAAA,IACF;AAGA,mBAAe,KAAK,MAAM,KAAK,KAAK,CAAC,GAAG;AAExC,WAAO,KAAK,IAAI,GAAG,UAAU;AAAA,EAC/B;AAAA;AAAA;AAAA;AAAA,EAKA,kBAAkB,MAAsB;AAEtC,WAAO,KAAK,KAAK,KAAK,SAAS,CAAC;AAAA,EAClC;AAAA;AAAA;AAAA;AAAA,EAKA,eAAe,MAAc,WAA4B;AACvD,QAAI,CAAC;AAAM,aAAO;AAElB,QAAI,CAAC,WAAW;AACd,aAAO,KAAK,kBAAkB,IAAI;AAAA,IACpC;AAEA,WAAO,KAAK,wBAAwB,MAAM,SAAS;AAAA,EACrD;AACF;;;AC7KO,IAAM,oBAAN,MAAwB;AAAA,EAK7B,YAAY,UAA8B;AAJ1C,SAAQ,QAAgC,oBAAI,IAAI;AAChD,SAAQ,cAAsB;AAI5B,SAAK,WAAW;AAAA,EAClB;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,aAA4B;AAChC,QAAI,KAAK,SAAS,iBAAiB;AACjC,cAAQ,IAAI,6CAAsC;AAAA,IACpD;AAGA,SAAK,cAAc;AAGnB,QAAI,KAAK,SAAS,sBAAsB,CAAC,KAAK,YAAY,GAAG;AAC3D,UAAI;AACF,cAAM,KAAK,iBAAiB;AAC5B,YAAI,KAAK,SAAS,iBAAiB;AACjC,kBAAQ,IAAI,+CAA0C;AAAA,QACxD;AAAA,MACF,SAAS,OAAO;AACd,YAAI,KAAK,SAAS,iBAAiB;AACjC,kBAAQ,KAAK,+DAAqD,KAAK;AAAA,QACzE;AAAA,MACF;AAAA,IACF,WAAW,KAAK,SAAS,iBAAiB;AACxC,cAAQ,IAAI,0DAAqD;AAAA,IACnE;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,mBAAkC;AACtC,QAAI,CAAC,KAAK,SAAS,oBAAoB;AACrC,YAAM,IAAI,MAAM,6CAA6C;AAAA,IAC/D;AAEA,QAAI;AACF,UAAI,KAAK,SAAS,iBAAiB;AACjC,gBAAQ,IAAI,yCAAkC,KAAK,SAAS,WAAW;AAAA,MACzE;AAEA,YAAM,WAAW,MAAM,MAAM,KAAK,SAAS,WAAW;AACtD,UAAI,CAAC,SAAS,IAAI;AAChB,cAAM,IAAI,MAAM,eAAe,SAAS,MAAM,EAAE;AAAA,MAClD;AAEA,YAAM,OAAO,MAAM,SAAS,KAAK;AAEjC,UAAI,CAAC,KAAK,QAAQ,CAAC,MAAM,QAAQ,KAAK,IAAI,GAAG;AAC3C,cAAM,IAAI,MAAM,yCAAyC;AAAA,MAC3D;AAGA,UAAI,iBAAiB;AACrB,iBAAW,SAAS,KAAK,MAAM;AAC7B,YAAI,MAAM,kBAAkB,MAAM,IAAI;AACpC,gBAAM,YAAuB;AAAA,YAC3B,IAAI,MAAM;AAAA,YACV,MAAM,MAAM,QAAQ,MAAM;AAAA,YAC1B,eAAe,MAAM;AAAA,YACrB,SAAS,MAAM,UAAU;AAAA,cACvB,QAAQ,MAAM,QAAQ;AAAA,cACtB,YAAY,MAAM,QAAQ;AAAA,YAC5B,IAAI;AAAA,UACN;AAGA,eAAK,MAAM,IAAI,MAAM,IAAI,SAAS;AAClC,eAAK,MAAM,IAAI,KAAK,mBAAmB,MAAM,EAAE,GAAG,SAAS;AAC3D,eAAK,MAAM,IAAI,KAAK,mBAAmB,MAAM,QAAQ,MAAM,EAAE,GAAG,SAAS;AAEzE;AAAA,QACF;AAAA,MACF;AAGA,YAAM,gBAAgB,KAAK,SAAS,kBAAkB,KAAK,KAAK;AAChE,WAAK,cAAc,KAAK,IAAI,IAAI;AAGhC,WAAK,YAAY;AAEjB,UAAI,KAAK,SAAS,iBAAiB;AACjC,gBAAQ,IAAI,oBAAe,cAAc,kBAAkB;AAAA,MAC7D;AAAA,IAEF,SAAS,OAAO;AACd,cAAQ,MAAM,wCAAmC,KAAK;AACtD,YAAM;AAAA,IACR;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,cAAc,WAAkC;AAC9C,QAAI,CAAC;AAAW,aAAO;AAGvB,UAAM,QAAQ,KAAK,MAAM,IAAI,SAAS;AACtC,QAAI;AAAO,aAAO,MAAM;AAGxB,UAAM,aAAa,KAAK,MAAM,IAAI,KAAK,mBAAmB,SAAS,CAAC;AACpE,QAAI;AAAY,aAAO,WAAW;AAGlC,UAAM,aAAa,KAAK,WAAW,SAAS;AAC5C,QAAI;AAAY,aAAO,WAAW;AAElC,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKA,aAAa,WAAqC;AAChD,QAAI,CAAC;AAAW,aAAO;AAGvB,UAAM,QAAQ,KAAK,MAAM,IAAI,SAAS;AACtC,QAAI;AAAO,aAAO;AAGlB,UAAM,aAAa,KAAK,MAAM,IAAI,KAAK,mBAAmB,SAAS,CAAC;AACpE,QAAI;AAAY,aAAO;AAGvB,WAAO,KAAK,WAAW,SAAS;AAAA,EAClC;AAAA;AAAA;AAAA;AAAA,EAKA,qBAAkC;AAChC,UAAM,eAAe,oBAAI,IAAuB;AAGhD,eAAW,SAAS,KAAK,MAAM,OAAO,GAAG;AACvC,mBAAa,IAAI,MAAM,IAAI,KAAK;AAAA,IAClC;AAEA,WAAO,MAAM,KAAK,aAAa,OAAO,CAAC,EAAE,KAAK,CAAC,GAAG,MAAM,EAAE,KAAK,cAAc,EAAE,IAAI,CAAC;AAAA,EACtF;AAAA;AAAA;AAAA;AAAA,EAKQ,mBAAmB,WAA2B;AACpD,WAAO,UACJ,YAAY,EACZ,QAAQ,cAAc,GAAG,EACzB,QAAQ,OAAO,GAAG,EAClB,QAAQ,UAAU,EAAE;AAAA,EACzB;AAAA;AAAA;AAAA;AAAA,EAKQ,WAAW,WAAqC;AACtD,UAAM,aAAa,KAAK,mBAAmB,SAAS;AAGpD,eAAW,CAAC,KAAK,KAAK,KAAK,KAAK,MAAM,QAAQ,GAAG;AAC/C,YAAM,gBAAgB,KAAK,mBAAmB,GAAG;AAGjD,UAAI,cAAc,SAAS,UAAU,KAAK,WAAW,SAAS,aAAa,GAAG;AAC5E,eAAO;AAAA,MACT;AAAA,IACF;AAEA,WAAO;AAAA,EACT;AAAA;AAAA;AAAA;AAAA,EAKQ,cAAuB;AAC7B,WAAO,KAAK,MAAM,OAAO,KAAK,KAAK,IAAI,IAAI,KAAK;AAAA,EAClD;AAAA;AAAA;AAAA;AAAA,EAKQ,cAAoB;AAC1B,QAAI;AACF,YAAM,YAAY;AAAA,QAChB,QAAQ,MAAM,KAAK,KAAK,MAAM,QAAQ,CAAC;AAAA,QACvC,QAAQ,KAAK;AAAA,MACf;AAEA,mBAAa,QAAQ,qBAAqB,KAAK,UAAU,SAAS,CAAC;AAAA,IACrE,SAAS,OAAO;AACd,cAAQ,KAAK,+BAA+B,KAAK;AAAA,IACnD;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKQ,gBAAsB;AAC5B,QAAI;AACF,YAAM,SAAS,aAAa,QAAQ,mBAAmB;AACvD,UAAI,CAAC;AAAQ;AAEb,YAAM,YAAY,KAAK,MAAM,MAAM;AAGnC,UAAI,KAAK,IAAI,IAAI,UAAU,QAAQ;AACjC,qBAAa,WAAW,mBAAmB;AAC3C;AAAA,MACF;AAGA,WAAK,QAAQ,IAAI,IAAI,UAAU,MAAM;AACrC,WAAK,cAAc,UAAU;AAE7B,cAAQ,IAAI,oBAAa,KAAK,MAAM,IAAI,oBAAoB;AAAA,IAE9D,SAAS,OAAO;AACd,cAAQ,KAAK,+BAA+B,KAAK;AACjD,mBAAa,WAAW,mBAAmB;AAAA,IAC7C;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,UAAyB;AAC7B,SAAK,MAAM,MAAM;AACjB,SAAK,cAAc;AACnB,UAAM,KAAK,iBAAiB;AAAA,EAC9B;AAAA;AAAA;AAAA;AAAA,EAKA,eAAe,UAAoC;AACjD,SAAK,WAAW;AAGhB,UAAM,gBAAgB,KAAK,SAAS,kBAAkB,KAAK,KAAK;AAChE,SAAK,cAAc,KAAK,IAAI,IAAI;AAAA,EAClC;AAAA;AAAA;AAAA;AAAA,EAKA,eAA4E;AAC1E,WAAO;AAAA,MACL,YAAY,KAAK,MAAM;AAAA,MACvB,WAAW,CAAC,KAAK,YAAY;AAAA,MAC7B,WAAW,IAAI,KAAK,KAAK,WAAW;AAAA,IACtC;AAAA,EACF;AACF;;;ACzQO,IAAM,kBAAN,MAAsB;AAAA,EAC3B,YACU,iBACA,mBACR;AAFQ;AACA;AAAA,EACP;AAAA;AAAA;AAAA;AAAA,EAKH,YAAY,MAAc,WAAmC;AAC3D,QAAI,CAAC,MAAM;AACT,aAAO;AAAA,QACL,YAAY;AAAA,QACZ,WAAW,aAAa;AAAA,QACxB,YAAY;AAAA,QACZ,YAAY;AAAA,QACZ,eAAe;AAAA,QACf,YAAY;AAAA,MACd;AAAA,IACF;AAGA,UAAM,aAAa,YACf,KAAK,gBAAgB,wBAAwB,MAAM,SAAS,IAC5D,KAAK,gBAAgB,eAAe,IAAI;AAG5C,UAAM,aAAa,YAAY,KAAK,kBAAkB,cAAc,SAAS,IAAI;AACjF,UAAM,YAAY,YAAY,KAAK,kBAAkB,aAAa,SAAS,IAAI;AAG/E,UAAM,aAAa,aAAc,aAAa,aAAc,MAAM;AAGlE,QAAI,gBAAgB;AACpB,QAAI,uCAAW,SAAS;AAEtB,sBAAiB,aAAa,MAAQ,UAAU,QAAQ;AAAA,IAC1D;AAEA,WAAO;AAAA,MACL;AAAA,MACA,WAAW,aAAa;AAAA,MACxB;AAAA,MACA;AAAA,MACA;AAAA,MACA,YAAY;AAAA;AAAA,IACd;AAAA,EACF;AAAA;AAAA;AAAA;AAAA,EAKA,cAAc,WAAkC;AAC9C,WAAO,KAAK,kBAAkB,cAAc,SAAS;AAAA,EACvD;AAAA;AAAA;AAAA;AAAA,EAKA,aAAa,WAAqC;AAChD,WAAO,KAAK,kBAAkB,aAAa,SAAS;AAAA,EACtD;AAAA;AAAA;AAAA;AAAA,EAKA,qBAAkC;AAChC,WAAO,KAAK,kBAAkB,mBAAmB;AAAA,EACnD;AAAA;AAAA;AAAA;AAAA,EAKA,cAAc,QAAgB,WAAmB,mBAA2B,GAAkB;AAC5F,UAAM,YAAY,KAAK,kBAAkB,aAAa,SAAS;AAC/D,QAAI,EAAC,uCAAW;AAAS,aAAO;AAEhC,UAAM,eAAe,SAAS;AAC9B,UAAM,aAAc,eAAe,MAAQ,UAAU,QAAQ;AAC7D,UAAM,iBAAkB,mBAAmB,MAAQ,UAAU,QAAQ;AAErE,WAAO,aAAa;AAAA,EACtB;AAAA;AAAA;AAAA;AAAA,EAMA,WAAW,MAAsB;AAC/B,WAAO,KAAK,gBAAgB,eAAe,IAAI;AAAA,EACjD;AAAA;AAAA;AAAA;AAAA,EAKA,MAAM,mBAAkC;AACtC,UAAM,KAAK,kBAAkB,QAAQ;AAAA,EACvC;AAAA;AAAA;AAAA;AAAA,EAKA,eAA4E;AAC1E,WAAO,KAAK,kBAAkB,aAAa;AAAA,EAC7C;AACF;;;AC3HA,sBAA+C;AAWxC,IAAM,mBAAuC;AAAA,EAClD,oBAAoB;AAAA,EACpB,aAAa;AAAA,EACb,iBAAiB;AAAA,EACjB,cAAc;AAAA,EACd,iBAAiB;AACnB;AAEO,IAAM,uBAAN,cAAmC,iCAAiB;AAAA,EAGzD,YAAY,KAAU,QAA0B;AAC9C,UAAM,KAAK,MAAM;AACjB,SAAK,SAAS;AAAA,EAChB;AAAA,EAEA,UAAgB;AA3BlB;AA4BI,UAAM,EAAE,YAAY,IAAI;AACxB,gBAAY,MAAM;AAElB,gBAAY,SAAS,MAAM,EAAE,MAAM,uBAAuB,CAAC;AAG3D,QAAI,wBAAQ,WAAW,EACpB,QAAQ,2BAA2B,EACnC,QAAQ,sFAAsF,EAC9F,UAAU,YAAU,OAClB,SAAS,KAAK,OAAO,SAAS,kBAAkB,EAChD,SAAS,OAAO,UAAU;AACzB,WAAK,OAAO,SAAS,qBAAqB;AAC1C,YAAM,KAAK,OAAO,aAAa;AAG/B,WAAK,QAAQ;AAAA,IACf,CAAC,CAAC;AAEN,QAAI,KAAK,OAAO,SAAS,oBAAoB;AAE3C,UAAI,wBAAQ,WAAW,EACpB,QAAQ,cAAc,EACtB,QAAQ,8DAA8D,EACtE,QAAQ,UAAQ,KACd,eAAe,qCAAqC,EACpD,SAAS,KAAK,OAAO,SAAS,WAAW,EACzC,SAAS,OAAO,UAAU;AACzB,aAAK,OAAO,SAAS,cAAc,SAAS,iBAAiB;AAC7D,cAAM,KAAK,OAAO,aAAa;AAAA,MACjC,CAAC,CAAC;AAGN,UAAI,wBAAQ,WAAW,EACpB,QAAQ,kBAAkB,EAC1B,QAAQ,6CAA6C,EACrD,UAAU,YAAU,OAClB,UAAU,GAAG,KAAK,CAAC,EACnB,SAAS,KAAK,OAAO,SAAS,eAAe,EAC7C,kBAAkB,EAClB,SAAS,OAAO,UAAU;AACzB,aAAK,OAAO,SAAS,kBAAkB;AACvC,cAAM,KAAK,OAAO,aAAa;AAAA,MACjC,CAAC,CAAC;AAAA,IACR;AAGA,QAAI,wBAAQ,WAAW,EACpB,QAAQ,eAAe,EACvB,QAAQ,kDAAkD,EAC1D,YAAY,cAAY,SACtB,UAAU,eAAe,6BAA6B,EACtD,UAAU,mBAAmB,sCAAsC,EACnE,SAAS,KAAK,OAAO,SAAS,YAAY,EAC1C,SAAS,OAAO,UAA6C;AAC5D,WAAK,OAAO,SAAS,eAAe;AACpC,YAAM,KAAK,OAAO,aAAa;AAAA,IACjC,CAAC,CAAC;AAGN,QAAI,wBAAQ,WAAW,EACpB,QAAQ,sBAAsB,EAC9B,QAAQ,+DAA+D,EACvE,UAAU,YAAU,OAClB,SAAS,KAAK,OAAO,SAAS,eAAe,EAC7C,SAAS,OAAO,UAAU;AACzB,WAAK,OAAO,SAAS,kBAAkB;AACvC,YAAM,KAAK,OAAO,aAAa;AAAA,IACjC,CAAC,CAAC;AAGN,gBAAY,SAAS,MAAM,EAAE,MAAM,eAAe,CAAC;AAEnD,UAAM,aAAY,UAAK,OAAO,QAAZ,mBAAiB;AACnC,QAAI,WAAW;AACb,YAAM,WAAW,YAAY,SAAS,OAAO,EAAE,KAAK,mBAAmB,CAAC;AAExE,eAAS,SAAS,KAAK;AAAA,QACrB,MAAM,kBAAkB,UAAU,UAAU;AAAA,MAC9C,CAAC;AAED,eAAS,SAAS,KAAK;AAAA,QACrB,MAAM,kBAAkB,UAAU,UAAU,eAAe,CAAC;AAAA,MAC9D,CAAC;AAED,UAAI,UAAU,WAAW;AACvB,iBAAS,SAAS,KAAK;AAAA,UACrB,MAAM;AAAA,UACN,KAAK;AAAA,QACP,CAAC;AAAA,MACH;AAGA,UAAI,wBAAQ,WAAW,EACpB,QAAQ,oBAAoB,EAC5B,QAAQ,8CAA8C,EACtD,UAAU,YAAU,OAClB,cAAc,aAAa,EAC3B,QAAQ,YAAY;AACnB,YAAI,KAAK,OAAO,KAAK;AACnB,cAAI;AACF,mBAAO,cAAc,eAAe;AACpC,mBAAO,YAAY,IAAI;AAEvB,kBAAM,KAAK,OAAO,IAAI,iBAAiB;AAGvC,iBAAK,QAAQ;AAAA,UACf,SAAS,OAAO;AACd,oBAAQ,MAAM,iCAAiC,KAAK;AACpD,mBAAO,cAAc,gBAAgB;AACrC,uBAAW,MAAM;AACf,qBAAO,cAAc,aAAa;AAClC,qBAAO,YAAY,KAAK;AAAA,YAC1B,GAAG,GAAI;AAAA,UACT;AAAA,QACF;AAAA,MACF,CAAC,CAAC;AAAA,IACR;AAAA,EACF;AACF;;;AJ9IA,IAAqB,mBAArB,cAA8C,wBAAO;AAAA,EAMnD,MAAM,SAAS;AAEb,UAAM,KAAK,aAAa;AAExB,QAAI,KAAK,SAAS,iBAAiB;AACjC,cAAQ,IAAI,yCAAkC;AAAA,IAChD;AAGA,SAAK,kBAAkB,IAAI,gBAAgB;AAC3C,SAAK,oBAAoB,IAAI,kBAAkB,KAAK,QAAQ;AAG5D,QAAI;AACF,YAAM,KAAK,kBAAkB,WAAW;AAAA,IAC1C,SAAS,OAAO;AACd,UAAI,KAAK,SAAS,iBAAiB;AACjC,gBAAQ,KAAK,2CAA2C,KAAK;AAAA,MAC/D;AAAA,IACF;AAGA,SAAK,MAAM,IAAI,gBAAgB,KAAK,iBAAiB,KAAK,iBAAiB;AAG3E,SAAK,cAAc,IAAI,qBAAqB,KAAK,KAAK,IAAI,CAAC;AAE3D,QAAI,KAAK,SAAS,iBAAiB;AACjC,cAAQ,IAAI,+CAA0C;AAGtD,YAAM,YAAY,KAAK,IAAI,aAAa;AACxC,cAAQ,IAAI,0BAAmB,UAAU,UAAU,4BAA4B,UAAU,UAAU,eAAe,CAAC,EAAE;AAAA,IACvH;AAAA,EACF;AAAA,EAEA,WAAW;AACT,QAAI,KAAK,SAAS,iBAAiB;AACjC,cAAQ,IAAI,uCAAgC;AAAA,IAC9C;AAAA,EACF;AAAA,EAEA,MAAM,eAAe;AACnB,SAAK,WAAW,OAAO,OAAO,CAAC,GAAG,kBAAkB,MAAM,KAAK,SAAS,CAAC;AAAA,EAC3E;AAAA,EAEA,MAAM,eAAe;AACnB,UAAM,KAAK,SAAS,KAAK,QAAQ;AAGjC,QAAI,KAAK,mBAAmB;AAC1B,WAAK,kBAAkB,eAAe,KAAK,QAAQ;AAAA,IACrD;AAAA,EACF;AACF;",
  "names": ["import_obsidian"]
}
